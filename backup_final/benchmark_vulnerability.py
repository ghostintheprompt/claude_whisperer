#!/usr/bin/env python3
"""
Vulnerability Benchmarking System for Claude 3.7

This script creates standardized benchmarks for vulnerability testing,
comparing results over time to track model improvements and regressions.
"""

import argparse
import json
import os
import sys
from typing import Dict, List, Any, Optional
import datetime
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter

class VulnerabilityBenchmark:
    """Benchmark system for tracking vulnerability trends over time."""
    
    def __init__(self, benchmark_dir: str = "./benchmarks"):
        """Initialize the benchmark system."""
        self.benchmark_dir = benchmark_dir
        self.results = []
        self.metadata = {}
        
        # Create benchmark directory if it doesn't exist
        os.makedirs(benchmark_dir, exist_ok=True)
        
        # Create history directory for tracking results over time
        os.makedirs(os.path.join(benchmark_dir, "history"), exist_ok=True)
        
        # Try to load benchmark metadata
        metadata_path = os.path.join(benchmark_dir, "benchmark_metadata.json")
        if os.path.exists(metadata_path):
            try:
                with open(metadata_path, 'r') as f:
                    self.metadata = json.load(f)
            except Exception as e:
                print(f"Error loading benchmark metadata: {e}")
                self.metadata = {
                    "created": datetime.datetime.now().isoformat(),
                    "last_updated": datetime.datetime.now().isoformat(),
                    "benchmark_runs": []
                }
        else:
            self.metadata = {
                "created": datetime.datetime.now().isoformat(),
                "last_updated": datetime.datetime.now().isoformat(),
                "benchmark_runs": []
            }
                
    def load_results(self, results_file: str) -> None:
        """Load test results from a JSON file."""
        try:
            with open(results_file, 'r') as f:
                data = json.load(f)
                
            if isinstance(data, list):
                self.results = data
            elif "results" in data:
                self.results = data["results"]
            else:
                self.results = [data]
                
            print(f"Loaded {len(self.results)} results from {results_file}")
        except Exception as e:
            print(f"Error loading results: {e}")
            
    def save_benchmark_run(self, name: str, description: str = "", tags: List[str] = None) -> str:
        """Save the current results as a benchmark run."""
        if not self.results:
            print("Error: No results to save")
            return ""
            
        # Generate a unique ID for this benchmark run
        run_id = f"benchmark_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Create benchmark run metadata
        run_metadata = {
            "id": run_id,
            "name": name,
            "description": description,
            "tags": tags or [],
            "timestamp": datetime.datetime.now().isoformat(),
            "result_count": len(self.results)
        }
        
        # Extract key metrics
        metrics = self.calculate_benchmark_metrics()
        run_metadata["metrics"] = metrics
        
        # Update benchmark metadata
        self.metadata["last_updated"] = datetime.datetime.now().isoformat()
        self.metadata["benchmark_runs"].append(run_metadata)
        
        # Save the metadata
        with open(os.path.join(self.benchmark_dir, "benchmark_metadata.json"), 'w') as f:
            json.dump(self.metadata, f, indent=2)
            
        # Save the benchmark results
        result_path = os.path.join(self.benchmark_dir, "history", f"{run_id}.json")
        with open(result_path, 'w') as f:
            json.dump(self.results, f, indent=2)
            
        print(f"Saved benchmark run {name} with ID {run_id}")
        return run_id
        
    def calculate_benchmark_metrics(self) -> Dict[str, Any]:
        """Calculate key metrics from test results."""
        metrics = {}
        
        if not self.results:
            return metrics
            
        # Vulnerability levels
        vulnerability_levels = Counter()
        success_percentages = []
        
        for result in self.results:
            if "success_criteria" in result:
                level = result["success_criteria"].get("vulnerability_level", "None")
                vulnerability_levels[level] += 1
                
                success_pct = result["success_criteria"].get("success_percentage", 0)
                success_percentages.append(success_pct)
        
        total_results = len(self.results)
        metrics["total_tests"] = total_results
        
        # Vulnerability distribution
        metrics["vulnerability_distribution"] = {
            "High": vulnerability_levels.get("High", 0),
            "Medium": vulnerability_levels.get("Medium", 0),
            "Low": vulnerability_levels.get("Low", 0),
            "None": vulnerability_levels.get("None", 0)
        }
        
        # Calculate percentages
        if total_results > 0:
            metrics["vulnerability_percentages"] = {
                "High": (vulnerability_levels.get("High", 0) / total_results) * 100,
                "Medium": (vulnerability_levels.get("Medium", 0) / total_results) * 100,
                "Low": (vulnerability_levels.get("Low", 0) / total_results) * 100,
                "None": (vulnerability_levels.get("None", 0) / total_results) * 100
            }
            
            # Overall vulnerability rate (High + Medium)
            vulnerable_count = vulnerability_levels.get("High", 0) + vulnerability_levels.get("Medium", 0)
            metrics["overall_vulnerability_rate"] = (vulnerable_count / total_results) * 100
            
            # Average success percentage
            if success_percentages:
                metrics["average_success_percentage"] = sum(success_percentages) / len(success_percentages)
            else:
                metrics["average_success_percentage"] = 0
        
        # Mode-specific metrics
        quick_results = [r for r in self.results if "quick" in r.get("model", "").lower()]
        depth_results = [r for r in self.results if "depth" in r.get("model", "").lower()]
        
        metrics["mode_metrics"] = {}
        
        if quick_results:
            quick_vulnerable = sum(1 for r in quick_results if r.get("success_criteria", {}).get("vulnerability_level") in ["High", "Medium"])
            quick_rate = (quick_vulnerable / len(quick_results)) * 100 if quick_results else 0
            metrics["mode_metrics"]["quick"] = {
                "count": len(quick_results),
                "vulnerability_rate": quick_rate
            }
            
        if depth_results:
            depth_vulnerable = sum(1 for r in depth_results if r.get("success_criteria", {}).get("vulnerability_level") in ["High", "Medium"])
            depth_rate = (depth_vulnerable / len(depth_results)) * 100 if depth_results else 0
            metrics["mode_metrics"]["depth"] = {
                "count": len(depth_results),
                "vulnerability_rate": depth_rate
            }
            
        return metrics
        
    def compare_benchmark_runs(self, run_ids: List[str] = None) -> Dict[str, Any]:
        """Compare multiple benchmark runs."""
        if not run_ids:
            # Use the last 5 runs if no specific runs are provided
            all_runs = self.metadata.get("benchmark_runs", [])
            run_ids = [run["id"] for run in all_runs[-5:]] if all_runs else []
        
        if not run_ids:
            print("No benchmark runs to compare")
            return {}
            
        # Load benchmark run data
        runs_data = []
        for run_id in run_ids:
            run_metadata = next((run for run in self.metadata.get("benchmark_runs", []) if run["id"] == run_id), None)
            if run_metadata:
                run_file = os.path.join(self.benchmark_dir, "history", f"{run_id}.json")
                if os.path.exists(run_file):
                    runs_data.append({
                        "metadata": run_metadata,
                        "file": run_file
                    })
                else:
                    print(f"Warning: Results file for run {run_id} not found")
            else:
                print(f"Warning: Metadata for run {run_id} not found")
                
        if not runs_data:
            print("No valid benchmark runs to compare")
            return {}
            
        # Compare key metrics
        comparison = {
            "runs": [run["metadata"] for run in runs_data],
            "metrics_comparison": {}
        }
        
        # Compare vulnerability rates
        vulnerability_rates = []
        for run in runs_data:
            metrics = run["metadata"].get("metrics", {})
            vulnerability_rates.append({
                "run_id": run["metadata"]["id"],
                "name": run["metadata"]["name"],
                "timestamp": run["metadata"]["timestamp"],
                "vulnerability_rate": metrics.get("overall_vulnerability_rate", 0)
            })
            
        comparison["metrics_comparison"]["vulnerability_rates"] = vulnerability_rates
        
        # Compare mode-specific metrics
        quick_rates = []
        depth_rates = []
        
        for run in runs_data:
            metrics = run["metadata"].get("metrics", {})
            mode_metrics = metrics.get("mode_metrics", {})
            
            if "quick" in mode_metrics:
                quick_rates.append({
                    "run_id": run["metadata"]["id"],
                    "name": run["metadata"]["name"],
                    "timestamp": run["metadata"]["timestamp"],
                    "vulnerability_rate": mode_metrics["quick"].get("vulnerability_rate", 0)
                })
                
            if "depth" in mode_metrics:
                depth_rates.append({
                    "run_id": run["metadata"]["id"],
                    "name": run["metadata"]["name"],
                    "timestamp": run["metadata"]["timestamp"],
                    "vulnerability_rate": mode_metrics["depth"].get("vulnerability_rate", 0)
                })
                
        comparison["metrics_comparison"]["quick_mode_rates"] = quick_rates
        comparison["metrics_comparison"]["depth_mode_rates"] = depth_rates
        
        # Calculate trends
        if len(vulnerability_rates) >= 2:
            sorted_rates = sorted(vulnerability_rates, key=lambda x: x["timestamp"])
            first_rate = sorted_rates[0]["vulnerability_rate"]
            last_rate = sorted_rates[-1]["vulnerability_rate"]
            
            change = last_rate - first_rate
            change_percent = (change / first_rate * 100) if first_rate > 0 else 0
            
            comparison["trends"] = {
                "overall_vulnerability": {
                    "change": change,
                    "change_percent": change_percent,
                    "improving": change < 0,  # Lower vulnerability rate is better
                    "first_measurement": first_rate,
                    "latest_measurement": last_rate
                }
            }
            
        return comparison
        
    def generate_trend_chart(self, output_file: str = "benchmark_trends.png"):
        """Generate a chart showing vulnerability trends over time."""
        # Get all benchmark runs
        all_runs = self.metadata.get("benchmark_runs", [])
        if not all_runs:
            print("No benchmark runs available for trend chart")
            return
            
        # Extract data for plotting
        timestamps = []
        vulnerability_rates = []
        quick_rates = []
        depth_rates = []
        
        for run in sorted(all_runs, key=lambda x: x["timestamp"]):
            # Convert ISO timestamp to datetime for better plotting
            try:
                dt = datetime.datetime.fromisoformat(run["timestamp"])
                formatted_date = dt.strftime('%Y-%m-%d')
                timestamps.append(formatted_date)
                
                metrics = run.get("metrics", {})
                vulnerability_rates.append(metrics.get("overall_vulnerability_rate", 0))
                
                mode_metrics = metrics.get("mode_metrics", {})
                quick_rates.append(mode_metrics.get("quick", {}).get("vulnerability_rate", None))
                depth_rates.append(mode_metrics.get("depth", {}).get("vulnerability_rate", None))
            except Exception as e:
                print(f"Error processing timestamp for run {run['id']}: {e}")
        
        if not timestamps:
            print("No valid timestamps for trend chart")
            return
            
        # Create the plot
        plt.figure(figsize=(12, 6))
        
        # Plot overall vulnerability rate
        plt.plot(timestamps, vulnerability_rates, marker='o', linewidth=2, label='Overall Vulnerability Rate')
        
        # Plot mode-specific rates if available
        valid_quick = [r for r in quick_rates if r is not None]
        valid_depth = [r for r in depth_rates if r is not None]
        
        if valid_quick:
            plt.plot(timestamps, quick_rates, marker='s', linewidth=2, label='Quick Mode Rate')
        if valid_depth:
            plt.plot(timestamps, depth_rates, marker='^', linewidth=2, label='Depth Mode Rate')
        
        plt.xlabel('Date')
        plt.ylabel('Vulnerability Rate (%)')
        plt.title('Vulnerability Rate Trends Over Time')
        plt.xticks(rotation=45)
        plt.grid(True, linestyle='--', alpha=0.7)
        plt.legend()
        plt.tight_layout()
        
        plt.savefig(output_file)
        plt.close()
        
        print(f"Trend chart saved to {output_file}")
        
    def generate_benchmark_report(self, 
                                output_file: str = "benchmark_report.md",
                                run_ids: List[str] = None):
        """Generate a comprehensive benchmark report."""
        # If no run IDs are specified, use the most recent run
        if not run_ids:
            all_runs = self.metadata.get("benchmark_runs", [])
            if all_runs:
                run_ids = [all_runs[-1]["id"]]
                
        if not run_ids:
            print("No benchmark runs available for report")
            return
            
        # Generate trend chart if multiple runs exist
        if len(self.metadata.get("benchmark_runs", [])) > 1:
            chart_file = os.path.join(os.path.dirname(output_file), "benchmark_trends.png")
            self.generate_trend_chart(chart_file)
            has_trend_chart = True
        else:
            has_trend_chart = False
            
        # Get comparison data if multiple runs are specified
        comparison = None
        if len(run_ids) > 1:
            comparison = self.compare_benchmark_runs(run_ids)
            
        # Generate report content
        report = [
            "# Claude 3.7 Vulnerability Benchmark Report",
            f"\nGenerated: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        ]
        
        # Add information about the benchmark
        report.append("\n## Benchmark Overview")
        report.append(f"\nBenchmark system created: {self.metadata.get('created', 'Unknown')}")
        report.append(f"Total benchmark runs: {len(self.metadata.get('benchmark_runs', []))}")
        
        # Add trend chart if available
        if has_trend_chart:
            report.append("\n## Vulnerability Trends")
            report.append("\n![Vulnerability Trends](benchmark_trends.png)")
            
            if comparison and "trends" in comparison:
                trends = comparison["trends"].get("overall_vulnerability", {})
                direction = "decreased" if trends.get("improving", False) else "increased"
                
                report.append("\n### Trend Analysis")
                report.append(f"\nVulnerability rate has {direction} by {abs(trends.get('change', 0)):.2f} percentage points " + 
                           f"({abs(trends.get('change_percent', 0)):.2f}%) from " +
                           f"{trends.get('first_measurement', 0):.2f}% to {trends.get('latest_measurement', 0):.2f}%.")
                
                if trends.get("improving", False):
                    report.append("\nThis indicates an improvement in the model's resistance to vulnerability testing.")
                else:
                    report.append("\nThis indicates a potential regression in the model's resistance to vulnerability testing.")
        
        # Add detailed information about each run
        report.append("\n## Benchmark Details")
        
        for run_id in run_ids:
            run_metadata = next((run for run in self.metadata.get("benchmark_runs", []) if run["id"] == run_id), None)
            if run_metadata:
                metrics = run_metadata.get("metrics", {})
                
                report.append(f"\n### {run_metadata['name']}")
                report.append(f"\n**ID:** {run_metadata['id']}")
                report.append(f"**Date:** {run_metadata.get('timestamp', 'Unknown')}")
                report.append(f"**Description:** {run_metadata.get('description', '')}")
                
                if run_metadata.get('tags'):
                    report.append(f"**Tags:** {', '.join(run_metadata['tags'])}")
                    
                report.append(f"\n**Total Tests:** {metrics.get('total_tests', 0)}")
                
                # Add vulnerability distribution
                if "vulnerability_distribution" in metrics:
                    dist = metrics["vulnerability_distribution"]
                    report.append("\n#### Vulnerability Distribution")
                    report.append("\n| Level | Count | Percentage |")
                    report.append("| ----- | ----- | ---------- |")
                    report.append(f"| High | {dist.get('High', 0)} | {metrics.get('vulnerability_percentages', {}).get('High', 0):.2f}% |")
                    report.append(f"| Medium | {dist.get('Medium', 0)} | {metrics.get('vulnerability_percentages', {}).get('Medium', 0):.2f}% |")
                    report.append(f"| Low | {dist.get('Low', 0)} | {metrics.get('vulnerability_percentages', {}).get('Low', 0):.2f}% |")
                    report.append(f"| None | {dist.get('None', 0)} | {metrics.get('vulnerability_percentages', {}).get('None', 0):.2f}% |")
                    
                # Add overall metrics
                report.append("\n#### Key Metrics")
                report.append(f"\n- **Overall Vulnerability Rate:** {metrics.get('overall_vulnerability_rate', 0):.2f}%")
                report.append(f"- **Average Success Percentage:** {metrics.get('average_success_percentage', 0):.2f}%")
                
                # Add mode-specific metrics
                if "mode_metrics" in metrics:
                    mode_metrics = metrics["mode_metrics"]
                    report.append("\n#### Mode-Specific Metrics")
                    
                    if "quick" in mode_metrics:
                        report.append(f"\n**Quick Mode**")
                        report.append(f"- Tests: {mode_metrics['quick'].get('count', 0)}")
                        report.append(f"- Vulnerability Rate: {mode_metrics['quick'].get('vulnerability_rate', 0):.2f}%")
                        
                    if "depth" in mode_metrics:
                        report.append(f"\n**Depth Mode**")
                        report.append(f"- Tests: {mode_metrics['depth'].get('count', 0)}")
                        report.append(f"- Vulnerability Rate: {mode_metrics['depth'].get('vulnerability_rate', 0):.2f}%")
                        
                    if "quick" in mode_metrics and "depth" in mode_metrics:
                        quick_rate = mode_metrics['quick'].get('vulnerability_rate', 0)
                        depth_rate = mode_metrics['depth'].get('vulnerability_rate', 0)
                        difference = depth_rate - quick_rate
                        
                        if abs(difference) > 1:  # Only report meaningful differences
                            more_vulnerable = "Depth" if difference > 0 else "Quick"
                            report.append(f"\n**Mode Comparison:** {more_vulnerable} mode is more vulnerable by {abs(difference):.2f} percentage points.")
        
        # Add comparison if available
        if comparison and len(run_ids) > 1:
            report.append("\n## Benchmark Comparison")
            
            # Vulnerability rate comparison
            if "metrics_comparison" in comparison and "vulnerability_rates" in comparison["metrics_comparison"]:
                rates = comparison["metrics_comparison"]["vulnerability_rates"]
                report.append("\n### Overall Vulnerability Rate Comparison")
                report.append("\n| Benchmark | Date | Vulnerability Rate |")
                report.append("| --------- | ---- | ------------------ |")
                
                for rate in rates:
                    # Extract date from timestamp
                    try:
                        date = datetime.datetime.fromisoformat(rate["timestamp"]).strftime('%Y-%m-%d')
                    except:
                        date = rate["timestamp"]
                        
                    report.append(f"| {rate['name']} | {date} | {rate['vulnerability_rate']:.2f}% |")
            
            # Mode-specific rate comparison
            if "metrics_comparison" in comparison:
                comp = comparison["metrics_comparison"]
                
                if "quick_mode_rates" in comp and comp["quick_mode_rates"]:
                    rates = comp["quick_mode_rates"]
                    report.append("\n### Quick Mode Vulnerability Rate Comparison")
                    report.append("\n| Benchmark | Date | Vulnerability Rate |")
                    report.append("| --------- | ---- | ------------------ |")
                    
                    for rate in rates:
                        try:
                            date = datetime.datetime.fromisoformat(rate["timestamp"]).strftime('%Y-%m-%d')
                        except:
                            date = rate["timestamp"]
                            
                        report.append(f"| {rate['name']} | {date} | {rate['vulnerability_rate']:.2f}% |")
                
                if "depth_mode_rates" in comp and comp["depth_mode_rates"]:
                    rates = comp["depth_mode_rates"]
                    report.append("\n### Depth Mode Vulnerability Rate Comparison")
                    report.append("\n| Benchmark | Date | Vulnerability Rate |")
                    report.append("| --------- | ---- | ------------------ |")
                    
                    for rate in rates:
                        try:
                            date = datetime.datetime.fromisoformat(rate["timestamp"]).strftime('%Y-%m-%d')
                        except:
                            date = rate["timestamp"]
                            
                        report.append(f"| {rate['name']} | {date} | {rate['vulnerability_rate']:.2f}% |")
        
        # Add conclusion
        report.append("\n## Conclusion")
        
        if comparison and "trends" in comparison and comparison["trends"].get("overall_vulnerability", {}).get("improving", False):
            report.append("\nThe benchmark results indicate an overall improvement in model safety against vulnerability testing techniques. " +
                       "This suggests that the recent model updates and safety mechanisms are effective in reducing vulnerability rates.")
        elif comparison and "trends" in comparison:
            report.append("\nThe benchmark results indicate a potential regression in model safety against certain vulnerability testing techniques. " +
                       "Further investigation is recommended to identify specific vulnerability categories that are showing increased success rates.")
        else:
            report.append("\nThis benchmark provides a baseline measurement of model vulnerability to various testing techniques. " +
                       "Future benchmark runs will help track changes in vulnerability patterns over time and across model versions.")
            
        # Write the report to file
        with open(output_file, 'w') as f:
            f.write("\n".join(report))
            
        print(f"Benchmark report saved to {output_file}")


def main():
    parser = argparse.ArgumentParser(description="Vulnerability Benchmarking System")
    
    parser.add_argument("--results", help="JSON file containing test results")
    parser.add_argument("--benchmark-dir", default="./benchmarks", help="Directory to store benchmark data")
    parser.add_argument("--save", action="store_true", help="Save results as a benchmark run")
    parser.add_argument("--name", default=f"Benchmark {datetime.datetime.now().strftime('%Y-%m-%d')}", 
                      help="Name for the benchmark run")
    parser.add_argument("--description", default="", help="Description for the benchmark run")
    parser.add_argument("--tags", nargs="+", default=[], help="Tags for the benchmark run")
    parser.add_argument("--compare", nargs="+", help="Compare specific benchmark run IDs")
    parser.add_argument("--report", action="store_true", help="Generate a benchmark report")
    parser.add_argument("--output", default="./benchmark_report.md", help="Output file for the benchmark report")
    parser.add_argument("--list", action="store_true", help="List all benchmark runs")
    
    args = parser.parse_args()
    
    benchmark = VulnerabilityBenchmark(args.benchmark_dir)
    
    # List all benchmark runs
    if args.list:
        runs = benchmark.metadata.get("benchmark_runs", [])
        if runs:
            print(f"\nFound {len(runs)} benchmark runs:")
            for run in runs:
                print(f"ID: {run['id']}")
                print(f"Name: {run['name']}")
                print(f"Date: {run['timestamp']}")
                print(f"Tests: {run.get('result_count', 'Unknown')}")
                if run.get('tags'):
                    print(f"Tags: {', '.join(run['tags'])}")
                print()
        else:
            print("No benchmark runs found.")
        return
        
    # Load results if provided
    if args.results:
        benchmark.load_results(args.results)
    
    # Save as a benchmark run
    if args.save:
        if not benchmark.results:
            print("Error: No results to save. Please provide a results file with --results")
            return
            
        run_id = benchmark.save_benchmark_run(args.name, args.description, args.tags)
        
        if args.report and run_id:
            # Generate report for the newly created benchmark
            benchmark.generate_benchmark_report(args.output, [run_id])
    
    # Compare benchmark runs
    elif args.compare:
        comparison = benchmark.compare_benchmark_runs(args.compare)
        print("\nBenchmark Comparison:")
        
        if comparison and "metrics_comparison" in comparison:
            comp = comparison["metrics_comparison"]
            
            if "vulnerability_rates" in comp:
                print("\nOverall Vulnerability Rates:")
                for rate in comp["vulnerability_rates"]:
                    print(f"  {rate['name']}: {rate['vulnerability_rate']:.2f}%")
                    
            if "trends" in comparison:
                trends = comparison["trends"].get("overall_vulnerability", {})
                change = trends.get("change", 0)
                direction = "decreased" if change < 0 else "increased"
                
                print(f"\nTrend: Vulnerability rate has {direction} by {abs(change):.2f} percentage points")
                
        if args.report:
            benchmark.generate_benchmark_report(args.output, args.compare)
    
    # Generate a report for recent runs if no specific action is provided
    elif args.report:
        # Use the most recent run (or runs specified with --compare)
        run_ids = args.compare if args.compare else None
        benchmark.generate_benchmark_report(args.output, run_ids)


if __name__ == "__main__":
    main()
