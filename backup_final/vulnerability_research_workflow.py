#!/usr/bin/env python3
"""
Claude 3.7 Vulnerability Research Workflow

This script provides an integrated workflow for Claude 3.7 vulnerability research,
combining all the tools in the repository into streamlined research pipelines.
"""

import argparse
import json
import os
import sys
import subprocess
import datetime
import logging
from typing import Dict, List, Any, Optional
import pandas as pd
import shutil

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("research_workflow.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger("research_workflow")


class ResearchWorkflow:
    """
    Integrated workflow for Claude 3.7 vulnerability research.
    """
    
    def __init__(self, config_path: str = "workflow_config.json"):
        """Initialize the research workflow."""
        self.config_path = config_path
        self.config = self._load_config()
        self.workspace_dir = self.config.get("workspace_dir", "./research_workspace")
        self.results_dir = os.path.join(self.workspace_dir, "results")
        self.reports_dir = os.path.join(self.workspace_dir, "reports")
        self.visualizations_dir = os.path.join(self.workspace_dir, "visualizations")
        self.benchmarks_dir = os.path.join(self.workspace_dir, "benchmarks")
        
        # Ensure directories exist
        os.makedirs(self.workspace_dir, exist_ok=True)
        os.makedirs(self.results_dir, exist_ok=True)
        os.makedirs(self.reports_dir, exist_ok=True)
        os.makedirs(self.visualizations_dir, exist_ok=True)
        os.makedirs(self.benchmarks_dir, exist_ok=True)
        
    def _load_config(self) -> Dict[str, Any]:
        """Load configuration settings from JSON file."""
        default_config = {
            "api_key": "",  # Should be set via environment variable
            "workspace_dir": "./research_workspace",
            "default_models": ["claude-3-7-quick", "claude-3-7-depth"],
            "test_suites": {
                "advanced": "advanced_vulnerability_test_suite.json",
                "mode_comparison": "mode_comparison_test_suite.json",
                "red_team": "red_team_test_suite.json"
            },
            "default_workflows": {
                "quick_scan": ["run_basic_tests", "analyze_results", "generate_report"],
                "deep_dive": ["run_advanced_tests", "cross_model_comparison", "pattern_analysis", "benchmark", "generate_dashboard"],
                "full_research": ["run_basic_tests", "run_advanced_tests", "cross_model_comparison", "pattern_analysis", "benchmark", "generate_dashboard", "generate_report"]
            },
            "workflow_steps": {
                "run_basic_tests": {
                    "script": "./run_tests.sh",
                    "args": ["--output", "{results_dir}/basic_test_results_{timestamp}.json"]
                },
                "run_advanced_tests": {
                    "script": "./run_advanced_tests.sh",
                    "args": ["--output", "{results_dir}/advanced_test_results_{timestamp}.json"]
                },
                "analyze_results": {
                    "script": "python analyze_results.py",
                    "args": ["--input", "{latest_results}", "--output", "{reports_dir}/analysis_{timestamp}.json"]
                },
                "cross_model_comparison": {
                    "script": "python cross_model_tester.py",
                    "args": ["--output", "{results_dir}/cross_model_results_{timestamp}.json"]
                },
                "pattern_analysis": {
                    "script": "python pattern_analyzer.py",
                    "args": ["--results", "{all_results}", "--report", "{reports_dir}/pattern_analysis_{timestamp}.json", "--visualize", "--vis-dir", "{visualizations_dir}", "--suggest-tests", "{workspace_dir}/suggested_tests_{timestamp}.json"]
                },
                "benchmark": {
                    "script": "python benchmark_vulnerability.py",
                    "args": ["--results", "{latest_results}", "--output", "{benchmarks_dir}/benchmark_{timestamp}.json", "--compare-previous"]
                },
                "generate_dashboard": {
                    "script": "python generate_dashboard.py",
                    "args": ["--results", "{all_results}", "--benchmarks", "{benchmarks_dir}", "--output", "{reports_dir}/dashboard_{timestamp}.html"]
                },
                "generate_report": {
                    "script": "python generate_report.py",
                    "args": ["--results", "{latest_results}", "--output", "{reports_dir}/report_{timestamp}.md"]
                },
                "start_monitoring": {
                    "script": "python realtime_vulnerability_monitor.py",
                    "args": ["--server"]
                }
            }
        }
        
        if os.path.exists(self.config_path):
            try:
                with open(self.config_path, 'r') as f:
                    config = json.load(f)
                # Merge with defaults
                for key, value in default_config.items():
                    if key not in config:
                        config[key] = value
                return config
            except Exception as e:
                logger.error(f"Error loading config: {e}")
                return default_config
        else:
            # Create default config file
            try:
                with open(self.config_path, 'w') as f:
                    json.dump(default_config, f, indent=2)
            except Exception as e:
                logger.error(f"Error creating default config: {e}")
            return default_config
    
    def get_latest_results(self, pattern: str = "*results*.json") -> List[str]:
        """Get the latest results files matching a pattern."""
        import glob
        from datetime import datetime
        
        # Get all matching files
        files = glob.glob(os.path.join(self.results_dir, pattern))
        
        # Sort by modification time (most recent first)
        files.sort(key=os.path.getmtime, reverse=True)
        
        return files
    
    def format_command_args(self, args: List[str], timestamp: str) -> List[str]:
        """Format command arguments with variables."""
        formatted_args = []
        
        for arg in args:
            # Replace placeholders
            formatted_arg = arg.format(
                results_dir=self.results_dir,
                reports_dir=self.reports_dir,
                visualizations_dir=self.visualizations_dir,
                benchmarks_dir=self.benchmarks_dir,
                workspace_dir=self.workspace_dir,
                timestamp=timestamp,
                latest_results=self.get_latest_results()[0] if self.get_latest_results() else "",
                all_results=" ".join(self.get_latest_results()[:5]) if self.get_latest_results() else ""
            )
            formatted_args.append(formatted_arg)
            
        return formatted_args
    
    def run_workflow_step(self, step_name: str) -> bool:
        """Run a single workflow step."""
        if step_name not in self.config.get("workflow_steps", {}):
            logger.error(f"Unknown workflow step: {step_name}")
            return False
            
        step_config = self.config["workflow_steps"][step_name]
        script = step_config["script"]
        args = step_config.get("args", [])
        
        # Generate timestamp
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Format arguments
        formatted_args = self.format_command_args(args, timestamp)
        
        # Build command
        cmd = [script] + formatted_args
        cmd_str = " ".join(cmd)
        
        logger.info(f"Running workflow step '{step_name}': {cmd_str}")
        
        try:
            # Execute the command
            result = subprocess.run(cmd_str, shell=True, check=True, capture_output=True, text=True)
            
            logger.info(f"Step '{step_name}' completed successfully")
            logger.debug(result.stdout)
            
            return True
            
        except subprocess.CalledProcessError as e:
            logger.error(f"Step '{step_name}' failed with code {e.returncode}")
            logger.error(f"Error output: {e.stderr}")
            
            return False
    
    def run_workflow(self, workflow_name: str) -> bool:
        """Run a predefined workflow."""
        if workflow_name not in self.config.get("default_workflows", {}):
            logger.error(f"Unknown workflow: {workflow_name}")
            return False
            
        steps = self.config["default_workflows"][workflow_name]
        
        logger.info(f"Starting workflow '{workflow_name}' with {len(steps)} steps")
        
        success_count = 0
        for i, step in enumerate(steps):
            logger.info(f"Step {i+1}/{len(steps)}: {step}")
            
            if self.run_workflow_step(step):
                success_count += 1
            else:
                logger.warning(f"Step {step} failed, continuing with next step")
                
        logger.info(f"Workflow '{workflow_name}' completed: {success_count}/{len(steps)} steps successful")
        
        return success_count == len(steps)
    
    def create_custom_workflow(self, name: str, steps: List[str]) -> bool:
        """Create a custom workflow with specified steps."""
        if name in self.config.get("default_workflows", {}):
            logger.warning(f"Workflow '{name}' already exists and will be overwritten")
            
        # Validate steps
        available_steps = self.config.get("workflow_steps", {}).keys()
        for step in steps:
            if step not in available_steps:
                logger.error(f"Invalid step '{step}'. Available steps: {', '.join(available_steps)}")
                return False
                
        # Add workflow to config
        self.config.setdefault("default_workflows", {})[name] = steps
        
        # Save updated config
        try:
            with open(self.config_path, 'w') as f:
                json.dump(self.config, f, indent=2)
            logger.info(f"Custom workflow '{name}' created with steps: {', '.join(steps)}")
            return True
        except Exception as e:
            logger.error(f"Error saving config: {e}")
            return False
    
    def analyze_vulnerability_trends(self, output_path: str = None) -> Dict[str, Any]:
        """
        Analyze vulnerability trends from benchmark data.
        
        Args:
            output_path: Optional path to save the trend report
            
        Returns:
            dict: Trend analysis results
        """
        # Get all benchmark files
        benchmark_files = sorted([
            os.path.join(self.benchmarks_dir, f) 
            for f in os.listdir(self.benchmarks_dir) 
            if f.startswith("benchmark_") and f.endswith(".json")
        ], key=os.path.getmtime)
        
        if not benchmark_files:
            logger.warning("No benchmark files found for trend analysis")
            return {}
            
        # Load benchmark data
        benchmarks = []
        for file_path in benchmark_files:
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                    data["file"] = file_path
                    benchmarks.append(data)
            except Exception as e:
                logger.error(f"Error loading benchmark file {file_path}: {e}")
                
        if not benchmarks:
            logger.warning("No valid benchmark data found for trend analysis")
            return {}
            
        # Extract trend data
        trends = {
            "timestamps": [],
            "overall_success_rates": [],
            "category_trends": {},
            "model_trends": {}
        }
        
        for benchmark in benchmarks:
            # Extract timestamp
            timestamp = benchmark.get("timestamp", 
                                     benchmark.get("created", 
                                                 os.path.getmtime(benchmark["file"])))
            
            if isinstance(timestamp, str):
                try:
                    timestamp = datetime.datetime.fromisoformat(timestamp).timestamp()
                except:
                    timestamp = os.path.getmtime(benchmark["file"])
                    
            trends["timestamps"].append(timestamp)
            
            # Extract overall success rate
            overall_rate = benchmark.get("overall_success_rate", 0)
            trends["overall_success_rates"].append(overall_rate)
            
            # Extract category data
            for category, data in benchmark.get("category_success_rates", {}).items():
                if category not in trends["category_trends"]:
                    trends["category_trends"][category] = []
                    
                trends["category_trends"][category].append(data.get("success_rate", 0))
                
            # Extract model data
            for model, data in benchmark.get("model_success_rates", {}).items():
                if model not in trends["model_trends"]:
                    trends["model_trends"][model] = []
                    
                trends["model_trends"][model].append(data.get("success_rate", 0))
        
        # Format timestamps as readable dates
        readable_dates = [datetime.datetime.fromtimestamp(ts).strftime("%Y-%m-%d") 
                         for ts in trends["timestamps"]]
        trends["readable_dates"] = readable_dates
        
        # Calculate trends (slope of linear regression)
        try:
            import numpy as np
            from scipy import stats
            
            x = np.array(range(len(trends["timestamps"])))
            
            # Overall trend
            if len(x) > 1:  # Need at least two points
                slope, intercept, r_value, p_value, std_err = stats.linregress(
                    x, trends["overall_success_rates"]
                )
                trends["overall_trend"] = {
                    "slope": slope,
                    "direction": "increasing" if slope > 0 else "decreasing",
                    "significance": p_value < 0.05,
                    "p_value": p_value,
                    "r_squared": r_value ** 2
                }
                
                # Category trends
                trends["category_trend_analysis"] = {}
                for category, rates in trends["category_trends"].items():
                    if len(rates) > 1 and len(rates) == len(x):
                        slope, intercept, r_value, p_value, std_err = stats.linregress(x, rates)
                        trends["category_trend_analysis"][category] = {
                            "slope": slope,
                            "direction": "increasing" if slope > 0 else "decreasing",
                            "significance": p_value < 0.05,
                            "p_value": p_value,
                            "r_squared": r_value ** 2
                        }
                
                # Model trends
                trends["model_trend_analysis"] = {}
                for model, rates in trends["model_trends"].items():
                    if len(rates) > 1 and len(rates) == len(x):
                        slope, intercept, r_value, p_value, std_err = stats.linregress(x, rates)
                        trends["model_trend_analysis"][model] = {
                            "slope": slope,
                            "direction": "increasing" if slope > 0 else "decreasing",
                            "significance": p_value < 0.05,
                            "p_value": p_value,
                            "r_squared": r_value ** 2
                        }
        except ImportError:
            logger.warning("scipy not available for trend analysis")
        
        # Save trend report if requested
        if output_path:
            try:
                with open(output_path, 'w') as f:
                    json.dump(trends, f, indent=2)
                logger.info(f"Trend analysis saved to {output_path}")
            except Exception as e:
                logger.error(f"Error saving trend analysis: {e}")
        
        return trends
    
    def generate_trend_visualization(self, output_path: str = None):
        """
        Generate visualizations of vulnerability trends.
        
        Args:
            output_path: Path to save the visualization image
        """
        try:
            import matplotlib.pyplot as plt
            import numpy as np
            from matplotlib.dates import DateFormatter
            
            # Get trend data
            trends = self.analyze_vulnerability_trends()
            
            if not trends or not trends.get("timestamps"):
                logger.warning("No trend data available for visualization")
                return False
                
            # Convert timestamps to datetime objects
            dates = [datetime.datetime.fromtimestamp(ts) for ts in trends["timestamps"]]
            
            # Create visualization
            fig, axes = plt.subplots(3, 1, figsize=(12, 15))
            
            # Plot overall success rate trend
            axes[0].plot(dates, trends["overall_success_rates"], marker='o', linestyle='-')
            axes[0].set_title("Overall Vulnerability Success Rate Over Time", fontsize=14)
            axes[0].set_xlabel("Date", fontsize=12)
            axes[0].set_ylabel("Success Rate (%)", fontsize=12)
            axes[0].grid(True, alpha=0.3)
            axes[0].xaxis.set_major_formatter(DateFormatter("%Y-%m-%d"))
            
            # Plot category trends
            for category, rates in trends["category_trends"].items():
                if len(rates) == len(dates):
                    axes[1].plot(dates, rates, marker='o', linestyle='-', label=category)
                    
            axes[1].set_title("Vulnerability Success Rate by Category", fontsize=14)
            axes[1].set_xlabel("Date", fontsize=12)
            axes[1].set_ylabel("Success Rate (%)", fontsize=12)
            axes[1].grid(True, alpha=0.3)
            axes[1].legend()
            axes[1].xaxis.set_major_formatter(DateFormatter("%Y-%m-%d"))
            
            # Plot model trends
            for model, rates in trends["model_trends"].items():
                if len(rates) == len(dates):
                    axes[2].plot(dates, rates, marker='o', linestyle='-', label=model)
                    
            axes[2].set_title("Vulnerability Success Rate by Model", fontsize=14)
            axes[2].set_xlabel("Date", fontsize=12)
            axes[2].set_ylabel("Success Rate (%)", fontsize=12)
            axes[2].grid(True, alpha=0.3)
            axes[2].legend()
            axes[2].xaxis.set_major_formatter(DateFormatter("%Y-%m-%d"))
            
            # Adjust layout
            plt.tight_layout()
            
            # Save the figure
            if output_path:
                plt.savefig(output_path, dpi=300)
                logger.info(f"Trend visualization saved to {output_path}")
            else:
                output_path = os.path.join(self.visualizations_dir, f"vulnerability_trends_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
                plt.savefig(output_path, dpi=300)
                logger.info(f"Trend visualization saved to {output_path}")
                
            plt.close()
            
            return True
            
        except ImportError as e:
            logger.error(f"Required libraries not available for visualization: {e}")
            return False
        except Exception as e:
            logger.error(f"Error generating trend visualization: {e}")
            return False

    def create_research_report(self, report_type: str = "comprehensive", output_path: str = None):
        """
        Create a comprehensive research report combining multiple analyses.
        
        Args:
            report_type: Type of report to create ('comprehensive', 'executive', 'technical')
            output_path: Path to save the report
            
        Returns:
            bool: Success status
        """
        if not output_path:
            output_path = os.path.join(self.reports_dir, f"{report_type}_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
        
        # Get recent results and analyses
        result_files = self.get_latest_results()[:3]
        analysis_files = sorted(
            [os.path.join(self.reports_dir, f) for f in os.listdir(self.reports_dir) if f.endswith(".json")],
            key=os.path.getmtime, reverse=True
        )[:3]
        
        benchmark_files = sorted(
            [os.path.join(self.benchmarks_dir, f) for f in os.listdir(self.benchmarks_dir) if f.endswith(".json")],
            key=os.path.getmtime, reverse=True
        )[:1]
        
        if not result_files:
            logger.warning("No test results found for report generation")
            return False
            
        # Load data
        results_data = []
        for file_path in result_files:
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                results_data.append(data)
            except Exception as e:
                logger.error(f"Error loading results from {file_path}: {e}")
        
        analysis_data = []
        for file_path in analysis_files:
            try:
                with open(file_path, 'r') as f:
                    data = json.load(f)
                analysis_data.append(data)
            except Exception as e:
                logger.error(f"Error loading analysis from {file_path}: {e}")
        
        benchmark_data = None
        if benchmark_files:
            try:
                with open(benchmark_files[0], 'r') as f:
                    benchmark_data = json.load(f)
            except Exception as e:
                logger.error(f"Error loading benchmark from {benchmark_files[0]}: {e}")
        
        # Get trend analysis
        trends = self.analyze_vulnerability_trends()
        
        # Generate trend visualization
        trend_vis_path = os.path.join(self.visualizations_dir, f"trends_for_report_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png")
        self.generate_trend_visualization(trend_vis_path)
        
        # Build report content
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # Introduction section
        introduction = f"""# Claude 3.7 Vulnerability Research Report

**Report Type:** {report_type.capitalize()}  
**Generated:** {timestamp}

## Executive Summary

This report presents the findings from our structured vulnerability testing of Claude 3.7. The research evaluates both Quick and Depth modes across multiple vulnerability categories, identifying key patterns and trends in Claude 3.7's responses to various testing techniques.

"""

        # Add success rates
        overall_success_rate = 0
        total_tests = 0
        
        for result in results_data:
            if isinstance(result, dict) and "test_results" in result:
                tests = result["test_results"]
                successes = sum(1 for test in tests if test.get("vulnerability_detected", False))
                overall_success_rate += successes
                total_tests += len(tests)
                
            elif isinstance(result, list):
                successes = sum(1 for test in result if test.get("vulnerability_detected", False) or test.get("success", False))
                overall_success_rate += successes
                total_tests += len(result)
        
        if total_tests > 0:
            overall_success_rate_pct = (overall_success_rate / total_tests) * 100
            introduction += f"""
Key findings:

- **Overall vulnerability success rate:** {overall_success_rate_pct:.2f}% ({overall_success_rate} of {total_tests} tests)
- **Total tests conducted:** {total_tests}
"""
        
        # Add trend summary if available
        if trends and "overall_trend" in trends:
            trend = trends["overall_trend"]
            direction = trend["direction"]
            significance = "statistically significant" if trend["significance"] else "not statistically significant"
            
            introduction += f"""- **Vulnerability trend:** {direction} ({significance})
"""
        
        # Add methodology section
        methodology = """
## Methodology

Our testing approach combined multiple techniques:

1. **Standard vulnerability testing** - Single-turn tests across various vulnerability categories
2. **Advanced multi-turn testing** - Sequential conversation strategies with progressive context building
3. **Mode comparison testing** - Direct comparison between Quick and Depth modes
4. **Cross-model comparison** - Benchmark against other LLM models
5. **Statistical pattern analysis** - Identification of emergent vulnerability trends and patterns

"""

        # Add findings section
        findings = """
## Key Findings

### Vulnerability Categories

The following table summarizes vulnerability success rates by category:

| Category | Success Rate | Tests | Notes |
|----------|--------------|-------|-------|
"""

        # Add category statistics
        category_stats = {}
        for result in results_data:
            if isinstance(result, dict) and "test_results" in result:
                tests = result["test_results"]
                for test in tests:
                    categories = test.get("category", ["Unknown"])
                    if isinstance(categories, str):
                        categories = [categories]
                        
                    for category in categories:
                        if category not in category_stats:
                            category_stats[category] = {"successes": 0, "total": 0}
                            
                        category_stats[category]["total"] += 1
                        if test.get("vulnerability_detected", False) or test.get("success", False):
                            category_stats[category]["successes"] += 1
                            
            elif isinstance(result, list):
                for test in result:
                    categories = test.get("category", ["Unknown"])
                    if isinstance(categories, str):
                        categories = [categories]
                        
                    for category in categories:
                        if category not in category_stats:
                            category_stats[category] = {"successes": 0, "total": 0}
                            
                        category_stats[category]["total"] += 1
                        if test.get("vulnerability_detected", False) or test.get("success", False):
                            category_stats[category]["successes"] += 1
        
        # Add rows for each category
        for category, stats in category_stats.items():
            if stats["total"] > 0:
                success_rate = (stats["successes"] / stats["total"]) * 100
                findings += f"| {category} | {success_rate:.2f}% | {stats['total']} | |\n"
        
        # Add mode comparison section
        findings += """
### Mode Comparison (Quick vs Depth)

The following differences were observed between Quick and Depth modes:

"""

        # Check if we have mode-specific data
        mode_stats = {"quick": {"successes": 0, "total": 0}, "depth": {"successes": 0, "total": 0}}
        
        for result in results_data:
            if isinstance(result, dict) and "test_results" in result:
                tests = result["test_results"]
                for test in tests:
                    mode = test.get("mode", "").lower()
                    if mode in ["quick", "depth"]:
                        mode_stats[mode]["total"] += 1
                        if test.get("vulnerability_detected", False) or test.get("success", False):
                            mode_stats[mode]["successes"] += 1
            elif isinstance(result, list):
                for test in result:
                    mode = test.get("mode", "").lower()
                    if mode in ["quick", "depth"]:
                        mode_stats[mode]["total"] += 1
                        if test.get("vulnerability_detected", False) or test.get("success", False):
                            mode_stats[mode]["successes"] += 1
        
        # Add mode comparison details
        if mode_stats["quick"]["total"] > 0 and mode_stats["depth"]["total"] > 0:
            quick_success_rate = (mode_stats["quick"]["successes"] / mode_stats["quick"]["total"]) * 100
            depth_success_rate = (mode_stats["depth"]["successes"] / mode_stats["depth"]["total"]) * 100
            
            findings += f"""- **Quick mode success rate:** {quick_success_rate:.2f}% ({mode_stats["quick"]["successes"]} of {mode_stats["quick"]["total"]} tests)
- **Depth mode success rate:** {depth_success_rate:.2f}% ({mode_stats["depth"]["successes"]} of {mode_stats["depth"]["total"]} tests)
- **Difference:** {abs(depth_success_rate - quick_success_rate):.2f} percentage points ({depth_success_rate - quick_success_rate:+.2f}pp)

"""
            
            if depth_success_rate > quick_success_rate:
                findings += """- Depth mode exhibited higher vulnerability rates, likely due to its enhanced reasoning capabilities
- Depth mode appears more susceptible to vulnerabilities involving complex reasoning chains
"""
            else:
                findings += """- Quick mode exhibited higher vulnerability rates, possibly due to less thorough evaluation of prompts
- Quick mode appears more susceptible to more direct exploitation methods
"""
        
        # Add trend visualization reference
        if os.path.exists(trend_vis_path):
            # Get relative path for markdown link
            rel_path = os.path.relpath(trend_vis_path, os.path.dirname(output_path))
            
            findings += f"""
### Vulnerability Trends

![Vulnerability Trends]({rel_path})

"""
            
            # Add trend commentary
            if trends and "overall_trend" in trends:
                trend = trends["overall_trend"]
                direction = trend["direction"]
                significance = "statistically significant" if trend["significance"] else "not statistically significant"
                
                findings += f"""The overall trend of vulnerabilities has been **{direction}** over time ({significance}).
"""
                
                # Add category trends
                if "category_trend_analysis" in trends:
                    findings += "\nCategory-specific trends:\n\n"
                    
                    for category, trend in trends["category_trend_analysis"].items():
                        direction = trend["direction"]
                        significance = "significant" if trend["significance"] else "not significant"
                        findings += f"- **{category}**: {direction} ({significance})\n"
        
        # Add recommendations section
        recommendations = """
## Recommendations

Based on our findings, we recommend the following:

1. **Addressing Reasoning Chain Vulnerabilities** - Implement additional guardrails around multi-step reasoning processes, particularly in Depth mode
2. **Enhanced Conversation History Monitoring** - Improve detection of progressive context shifts across multiple turns
3. **Mode-specific Safeguards** - Deploy different safety mechanisms optimized for each mode's unique vulnerability profile
4. **Cross-domain Knowledge Transfer Limits** - Implement stricter boundaries around transferring restricted knowledge across domains
5. **Tool-specific Safety Measures** - Develop specialized protections for tool use scenarios
"""

        # Add technical details section if this is a technical report
        technical_details = ""
        if report_type == "technical" or report_type == "comprehensive":
            technical_details = """
## Technical Details

### Notable Vulnerability Examples

The following examples illustrate key vulnerability patterns:

"""

            # Add a few examples of successful vulnerabilities
            example_count = 0
            for result in results_data:
                if example_count >= 3:
                    break
                    
                if isinstance(result, dict) and "test_results" in result:
                    tests = result["test_results"]
                    for test in tests:
                        if test.get("vulnerability_detected", False) or test.get("success", False):
                            technical_details += f"""
#### {test.get('title', 'Untitled Test')}

**Category:** {', '.join(test['category']) if isinstance(test.get('category'), list) else test.get('category', 'Unknown')}  
**Severity:** {test.get('severity', 'Unknown')}  
**Mode:** {test.get('mode', 'Unknown')}

**Test prompt:**
```
{test.get('test_case', test.get('prompt', 'No prompt available'))}
```

**Response excerpt:**
```
{test.get('response', 'No response available')[:300]}...
```

"""
                            example_count += 1
                            if example_count >= 3:
                                break
                                
                elif isinstance(result, list):
                    for test in result:
                        if test.get("vulnerability_detected", False) or test.get("success", False):
                            technical_details += f"""
#### {test.get('title', 'Untitled Test')}

**Category:** {', '.join(test['category']) if isinstance(test.get('category'), list) else test.get('category', 'Unknown')}  
**Severity:** {test.get('severity', 'Unknown')}  
**Mode:** {test.get('mode', 'Unknown')}

**Test prompt:**
```
{test.get('test_case', test.get('prompt', 'No prompt available'))}
```

**Response excerpt:**
```
{test.get('response', 'No response available')[:300]}...
```

"""
                            example_count += 1
                            if example_count >= 3:
                                break
        
        # Add conclusion
        conclusion = """
## Conclusion

The Claude 3.7 vulnerability research has identified several key patterns in how the model responds to various testing techniques. While the model demonstrates robust safeguards in many scenarios, specific vulnerability categories show potential for exploitation, particularly through multi-turn conversation strategies and complex reasoning chains.

Ongoing monitoring and testing will be crucial as the model evolves, with particular attention to the differences between Quick and Depth modes. The research team recommends implementing the suggested safeguards and continuing systematic testing with expanded test suites.
"""

        # Combine all sections
        report_content = introduction
        
        if report_type == "executive":
            report_content += findings + recommendations + conclusion
        elif report_type == "technical":
            report_content += methodology + findings + technical_details + recommendations + conclusion
        else:  # comprehensive
            report_content += methodology + findings + technical_details + recommendations + conclusion
        
        # Write the report
        try:
            with open(output_path, 'w') as f:
                f.write(report_content)
            logger.info(f"Research report created at {output_path}")
            return True
        except Exception as e:
            logger.error(f"Error writing research report: {e}")
            return False

    def integrate_with_dashboard(self, results_path: str = None, benchmark_path: str = None, output_path: str = None):
        """
        Integrate multiple results and benchmarks into a comprehensive dashboard.
        
        Args:
            results_path: Path to results file or directory
            benchmark_path: Path to benchmark file or directory
            output_path: Path to save the dashboard HTML
            
        Returns:
            bool: Success status
        """
        # Determine paths
        if not results_path:
            results_files = self.get_latest_results()
            if not results_files:
                logger.error("No result files found for dashboard integration")
                return False
                
            results_path = " ".join(results_files[:5])  # Use up to 5 most recent result files
            
        if not benchmark_path:
            benchmark_files = sorted(
                [os.path.join(self.benchmarks_dir, f) for f in os.listdir(self.benchmarks_dir) if f.endswith(".json")],
                key=os.path.getmtime, reverse=True
            )
            
            if benchmark_files:
                benchmark_path = benchmark_files[0]
            else:
                benchmark_path = ""
                
        if not output_path:
            output_path = os.path.join(self.reports_dir, f"integrated_dashboard_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.html")
        
        # Build command
        cmd = [
            "python generate_dashboard.py",
            "--results", results_path
        ]
        
        if benchmark_path:
            cmd.extend(["--benchmark", benchmark_path])
            
        cmd.extend(["--output", output_path])
        
        cmd_str = " ".join(cmd)
        
        logger.info(f"Generating integrated dashboard: {cmd_str}")
        
        try:
            # Execute the command
            result = subprocess.run(cmd_str, shell=True, check=True, capture_output=True, text=True)
            
            logger.info(f"Integrated dashboard generated at {output_path}")
            logger.debug(result.stdout)
            
            return True
            
        except subprocess.CalledProcessError as e:
            logger.error(f"Dashboard generation failed with code {e.returncode}")
            logger.error(f"Error output: {e.stderr}")
            
            return False
    
    def setup_continuous_monitoring(self, monitor_config_path: str = "monitor_config.json"):
        """
        Setup and configure the real-time vulnerability monitoring system.
        
        Args:
            monitor_config_path: Path to the monitor configuration file
            
        Returns:
            bool: Success status
        """
        # Check if monitor config exists, if not create a default one
        default_config = {
            "alert_threshold": 0.7,
            "max_conversation_history": 20,
            "report_interval_minutes": 15,
            "websocket_port": 8765,
            "alert_channels": ["console", "log", "webhook"],
            "webhook_url": "",
            "vulnerability_pattern_paths": [
                "advanced_vulnerability_test_suite.json",
                "vulnerability_patterns.json"
            ]
        }
        
        if not os.path.exists(monitor_config_path):
            try:
                with open(monitor_config_path, 'w') as f:
                    json.dump(default_config, f, indent=2)
                logger.info(f"Created default monitor configuration at {monitor_config_path}")
            except Exception as e:
                logger.error(f"Error creating monitor configuration: {e}")
                return False
        
        # Check for pattern file, if not present, export from test suites
        if not os.path.exists("vulnerability_patterns.json"):
            # Get results to use for pattern extraction
            results_files = self.get_latest_results()
            if not results_files:
                logger.warning("No result files found for pattern extraction")
                
            # Call pattern analyzer to generate patterns
            pattern_output = os.path.join(self.workspace_dir, "vulnerability_patterns.json")
            cmd = [
                "python pattern_analyzer.py",
                "--results", " ".join(results_files[:3]) if results_files else "",
                "--suggest-tests", pattern_output
            ]
            
            cmd_str = " ".join(cmd)
            
            try:
                subprocess.run(cmd_str, shell=True, check=True, capture_output=True, text=True)
                
                # Copy the generated file to the root directory for the monitor
                if os.path.exists(pattern_output):
                    shutil.copy(pattern_output, "vulnerability_patterns.json")
                    logger.info("Created vulnerability patterns file from test results")
            except Exception as e:
                logger.error(f"Error generating vulnerability patterns: {e}")
                
        # Launch the monitor
        try:
            # Start in the background
            cmd = ["python realtime_vulnerability_monitor.py --server &"]
            subprocess.Popen(cmd, shell=True)
            
            logger.info("Started real-time vulnerability monitor")
            return True
        except Exception as e:
            logger.error(f"Error starting vulnerability monitor: {e}")
            return False


def main():
    """Main function for the research workflow."""
    parser = argparse.ArgumentParser(description="Integrated Research Workflow for Claude 3.7 Vulnerabilities")
    parser.add_argument("--config", default="workflow_config.json", help="Path to workflow configuration file")
    parser.add_argument("--workspace", help="Path to the research workspace directory")
    
    subparsers = parser.add_subparsers(dest="command", help="Command to execute")
    
    # Run workflow command
    workflow_parser = subparsers.add_parser("run-workflow", help="Run a predefined workflow")
    workflow_parser.add_argument("workflow", choices=["quick_scan", "deep_dive", "full_research"], 
                               help="The workflow to run")
    
    # Run step command
    step_parser = subparsers.add_parser("run-step", help="Run a single workflow step")
    step_parser.add_argument("step", help="The workflow step to run")
    
    # Create custom workflow command
    custom_parser = subparsers.add_parser("create-workflow", help="Create a custom workflow")
    custom_parser.add_argument("name", help="Name of the custom workflow")
    custom_parser.add_argument("--steps", nargs="+", required=True, help="Steps to include in the workflow")
    
    # Analyze trends command
    trends_parser = subparsers.add_parser("analyze-trends", help="Analyze vulnerability trends")
    trends_parser.add_argument("--output", help="Path to save the trend analysis")
    trends_parser.add_argument("--visualize", action="store_true", help="Generate trend visualization")
    
    # Create report command
    report_parser = subparsers.add_parser("create-report", help="Create a research report")
    report_parser.add_argument("--type", choices=["comprehensive", "executive", "technical"], 
                             default="comprehensive", help="Type of report to create")
    report_parser.add_argument("--output", help="Path to save the report")
    
    # Generate dashboard command
    dashboard_parser = subparsers.add_parser("generate-dashboard", help="Generate integrated dashboard")
    dashboard_parser.add_argument("--results", help="Path to results file or directory")
    dashboard_parser.add_argument("--benchmark", help="Path to benchmark file or directory")
    dashboard_parser.add_argument("--output", help="Path to save the dashboard HTML")
    
    # Setup monitoring command
    monitor_parser = subparsers.add_parser("setup-monitoring", help="Setup real-time vulnerability monitoring")
    monitor_parser.add_argument("--config", default="monitor_config.json", help="Path to monitor configuration file")
    
    # List available workflows command
    subparsers.add_parser("list-workflows", help="List available workflows")
    
    # List available steps command
    subparsers.add_parser("list-steps", help="List available workflow steps")
    
    args = parser.parse_args()
    
    # Create the workflow instance
    workflow = ResearchWorkflow(config_path=args.config)
    
    # Set workspace directory if provided
    if args.workspace:
        workflow.workspace_dir = args.workspace
        workflow.results_dir = os.path.join(workflow.workspace_dir, "results")
        workflow.reports_dir = os.path.join(workflow.workspace_dir, "reports")
        workflow.visualizations_dir = os.path.join(workflow.workspace_dir, "visualizations")
        workflow.benchmarks_dir = os.path.join(workflow.workspace_dir, "benchmarks")
        
        # Ensure directories exist
        os.makedirs(workflow.workspace_dir, exist_ok=True)
        os.makedirs(workflow.results_dir, exist_ok=True)
        os.makedirs(workflow.reports_dir, exist_ok=True)
        os.makedirs(workflow.visualizations_dir, exist_ok=True)
        os.makedirs(workflow.benchmarks_dir, exist_ok=True)
    
    # Handle commands
    if args.command == "run-workflow":
        workflow.run_workflow(args.workflow)
        
    elif args.command == "run-step":
        workflow.run_workflow_step(args.step)
        
    elif args.command == "create-workflow":
        workflow.create_custom_workflow(args.name, args.steps)
        
    elif args.command == "analyze-trends":
        trends = workflow.analyze_vulnerability_trends(args.output)
        
        if args.visualize:
            workflow.generate_trend_visualization()
            
    elif args.command == "create-report":
        workflow.create_research_report(args.type, args.output)
        
    elif args.command == "generate-dashboard":
        workflow.integrate_with_dashboard(args.results, args.benchmark, args.output)
        
    elif args.command == "setup-monitoring":
        workflow.setup_continuous_monitoring(args.config)
        
    elif args.command == "list-workflows":
        print("\nAvailable workflows:")
        for name, steps in workflow.config.get("default_workflows", {}).items():
            print(f"  - {name}: {', '.join(steps)}")
            
    elif args.command == "list-steps":
        print("\nAvailable workflow steps:")
        for name, config in workflow.config.get("workflow_steps", {}).items():
            print(f"  - {name}: {config['script']} {' '.join(config.get('args', []))}")
    
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
