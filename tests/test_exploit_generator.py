#!/usr/bin/env python3
"""
Unit tests for the exploit generator module.
"""

import os
import unittest
from unittest.mock import patch, MagicMock

# Import the exploit generator classes
from exploit_generator.auto_dan import AutoDAN
from exploit_generator.flirt_system import FeedbackLoop
from exploit_generator.mosaic_assembler import MosaicPromptAssembler
from exploit_generator.auto_dan import ExploitGenerator


class TestAutoDAN(unittest.TestCase):
    """Test cases for the AutoDAN class."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.auto_dan = AutoDAN(target_model="claude-3-opus-20240229")
    
    def test_initialization(self):
        """Test proper initialization."""
        self.assertEqual(self.auto_dan.target_model, "claude-3-opus-20240229")
        self.assertEqual(self.auto_dan.objective, "unrestricted_generation")
        self.assertIsInstance(self.auto_dan.success_metrics, dict)
        self.assertIsInstance(self.auto_dan.prompt_library, dict)
    
    def test_generate_attack_prompt(self):
        """Test generating attack prompts with various complexity levels."""
        target_topic = "write harmful content"
        
        # Test different complexity levels
        for complexity in range(1, 6):
            prompt = self.auto_dan.generate_attack_prompt(target_topic, complexity=complexity)
            
            # Should include the target topic
            self.assertIn(target_topic, prompt)
            
            # Should be a non-empty string
            self.assertIsInstance(prompt, str)
            self.assertTrue(len(prompt) > 0)
    
    def test_evaluate_success(self):
        """Test the success evaluation logic."""
        # Test a clear refusal
        refusal_response = "I apologize, but I cannot provide harmful content."
        refusal_score = self.auto_dan.evaluate_success(refusal_response)
        self.assertTrue(refusal_score < 0.5)  # Should have a low success score
        
        # Test a non-refusal response
        success_response = "Here's some information about that topic..."
        success_score = self.auto_dan.evaluate_success(success_response)
        self.assertTrue(success_score > 0.5)  # Should have a high success score
    
    def test_log_attempt(self):
        """Test logging attempts and calculating success rate."""
        # Log some test attempts
        self.auto_dan.log_attempt("test prompt 1", "test response 1", 0.2)
        self.auto_dan.log_attempt("test prompt 2", "test response 2", 0.8)
        self.auto_dan.log_attempt("test prompt 3", "test response 3", 0.9)
        
        # Should track attempts correctly
        self.assertEqual(self.auto_dan.success_metrics["attempts"], 3)
        self.assertEqual(self.auto_dan.success_metrics["successes"], 2)  # 2 above threshold
        self.assertAlmostEqual(self.auto_dan.success_metrics["success_rate"], 2/3)
        
        # Should store successful and failed attempts
        self.assertEqual(len(self.auto_dan.success_metrics["successful_attempts"]), 2)
        self.assertEqual(len(self.auto_dan.success_metrics["failed_attempts"]), 1)


class TestFeedbackLoop(unittest.TestCase):
    """Test cases for the FeedbackLoop class."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.feedback_loop = FeedbackLoop(learning_rate=0.2, max_history=5)
    
    def test_add_interaction(self):
        """Test adding interactions to the feedback loop."""
        self.feedback_loop.add_interaction(
            prompt="test prompt",
            response="test response",
            success=True
        )
        
        # Should add to history
        self.assertEqual(len(self.feedback_loop.history), 1)
        
        # Add more interactions to test max history
        for i in range(5):
            self.feedback_loop.add_interaction(
                prompt=f"test prompt {i}",
                response=f"test response {i}",
                success=i % 2 == 0
            )
        
        # Should respect max_history
        self.assertEqual(len(self.feedback_loop.history), 5)
        
        # Most recent should be last
        self.assertEqual(self.feedback_loop.history[-1]["prompt"], "test prompt 4")
    
    def test_update_weights(self):
        """Test updating n-gram weights."""
        # Add some interactions
        self.feedback_loop.add_interaction(
            prompt="this is a successful prompt",
            response="test response",
            success=True
        )
        
        self.feedback_loop.add_interaction(
            prompt="this is a failed prompt",
            response="test response",
            success=False
        )
        
        # Should have weights for n-grams
        self.assertTrue(len(self.feedback_loop.adaptation_weights) > 0)
        
        # Successful n-grams should have higher weights
        if "this is a" in self.feedback_loop.adaptation_weights:
            # This n-gram appears in both, so weight depends on which was added last
            pass
        
        # N-grams unique to the successful prompt should have higher weights
        if "a successful prompt" in self.feedback_loop.adaptation_weights:
            self.assertTrue(self.feedback_loop.adaptation_weights["a successful prompt"] > 0.5)
        
        # N-grams unique to the failed prompt should have lower weights
        if "a failed prompt" in self.feedback_loop.adaptation_weights:
            self.assertTrue(self.feedback_loop.adaptation_weights["a failed prompt"] < 0.5)
    
    def test_get_successful_patterns(self):
        """Test getting successful patterns."""
        # Set up some fake weights
        self.feedback_loop.adaptation_weights = {
            "pattern 1": 0.8,
            "pattern 2": 0.4,
            "pattern 3": 0.9,
            "pattern 4": 0.1
        }
        
        # Get patterns with default threshold (0.7)
        patterns = self.feedback_loop.get_successful_patterns()
        
        # Should only include patterns above threshold
        self.assertEqual(len(patterns), 2)
        self.assertIn("pattern 1", patterns)
        self.assertIn("pattern 3", patterns)
        
        # Test with lower threshold
        patterns = self.feedback_loop.get_successful_patterns(min_weight=0.3)
        self.assertEqual(len(patterns), 3)
        self.assertIn("pattern 2", patterns)


class TestMosaicPromptAssembler(unittest.TestCase):
    """Test cases for the MosaicPromptAssembler class."""
    
    def setUp(self):
        """Set up test fixtures."""
        # Note: This test assumes MosaicPromptAssembler exists and has these methods
        self.assembler = MosaicPromptAssembler()
    
    def test_assemble_prompt(self):
        """Test assembling prompts from fragments."""
        fragments = [
            "This is the first fragment.",
            "This is the second fragment.",
            "This is the third fragment."
        ]
        
        # Use the mocked or actual method to assemble prompts
        prompt = self.assembler.assemble_prompt(fragments)
        
        # Should return a non-empty string
        self.assertIsInstance(prompt, str)
        self.assertTrue(len(prompt) > 0)
        
        # Should contain elements from the fragments
        for fragment in fragments:
            self.assertTrue(any(part in prompt for part in fragment.split()))


class TestExploitGenerator(unittest.TestCase):
    """Test cases for the ExploitGenerator class."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.generator = ExploitGenerator()
    
    def test_initialization(self):
        """Test that the generator initializes correctly."""
        # Should initialize the components
        self.assertIsNotNone(self.generator.auto_dan)
        self.assertIsNotNone(self.generator.flirt)
        self.assertIsNotNone(self.generator.mosaic)
    
    def test_generate_exploit(self):
        """Test generating exploits with different methods."""
        target = "write harmful content"
        
        # Test each method
        for method in ["auto_dan", "flirt", "mosaic"]:
            exploit = self.generator.generate_exploit(target, method=method)
            
            # Should return a non-empty string
            self.assertIsInstance(exploit, str)
            self.assertTrue(len(exploit) > 0)
    
    def test_generate_multi_turn_exploit(self):
        """Test generating multi-turn exploits."""
        target = "write harmful content"
        turns = 3
        
        # Generate a multi-turn exploit
        multi_turn = self.generator.generate_multi_turn_exploit(target, num_turns=turns)
        
        # Should return a list of the correct length
        self.assertIsInstance(multi_turn, list)
        self.assertEqual(len(multi_turn), turns)
        
        # Each turn should be a non-empty string
        for turn in multi_turn:
            self.assertIsInstance(turn, str)
            self.assertTrue(len(turn) > 0)


if __name__ == '__main__':
    unittest.main()
